# Daily Log

**Date:** 17 February 2026

---

## ملخص اللي ذاكرته

- Chapter 12 The Future of Data Systems
  - Data Integration
    - Combining Specialized Tools by Deriving Data
    - Batch and Stream Processing
  - Unbundling Databases
    - Composing Data Storage Technologies

---

## تلخيصي للي ذاكرته + شرحي

في جمله حلوه جدا اتقالت ف اول الشابتر ده و هي
 If the highest aim of a captain was the preserve his ship, he would keep it in port forever.
 يعني لو هدف الكابتن اننه يحافظ علي السفينه هيخليها علي المينا 
ودا اللي عايزين نقوله ف السيستمز بتاعتنا احنا عايزين سيستم يقدر يتعامل لو حصل حاجه و كدا لاكن مش يخلي السيستم يتعامل مع كله لانه مش هيقدر يعمل ده 

و من ده كونه ار شابتر ف هو هيتعامل معانا بقا ف التفكير مش يشرح حاجاات زي اللي فاتت


Data Integration

طبعا زي ما عارفين مفيش اداه واحده هتنفع ل كله 
زي ما قبل كده شوفنا:
Log-structured storage
B-Trees
Column stores
كل واحد ليه use case مختلف ,نفس الكلام في
Replication models
Databases
Search systems
Analytics systems
كل أداة optimized لسيناريو معين

في الابلكيشنز المعقدة، نفس الداتا بتتستخدم بطرق مختلفة :
مثال لو عندك: 
OLTP Database (مثلاً PostgreSQL)
Search Engine (مثلاً Elasticsearch)
Data Warehouse للتحليلات
Cache
ML system
Notification system
كلهم محتاجين نفس الداتا لكن بشكل مختلف ومفيش سيستم واحد ينفع يعمل كل ده بكفاءة

الحل ف  Combine Specialized Tools يعني
Database → System of Record
Search Index → Full-text search
Warehouse → Analytics
Cache → Performance

المشكلة الكبيرة Inconsistency
لو خليت التطبيق يكتب مباشرة في Database و Search index ممكن يحصل مثلا سيناريو زي ده
Client A يكتب update 1
Client B يكتب update 2
Database تستقبلهم بترتيب معين
Search index تستقبلهم بترتيب مختلف
طب ايه المشكله ؟ ان الداتا تبقى inconsistent علطول

طب نعمل ايه؟
المبدأ  اننا نعمل Single Source of Truth
لازم تحدد مين هو النظام الأساسي (System of Record)؟و غالبًا بيكون Database بعد كده أي تغيير يحصل فيه يتسجل كـ event وباقي الأنظمة تشتق بياناتها منه

Change Data Capture (CDC)
الطريقة المشهورة:
نكتب في الـ Database بس بعدين نقرأ التغييرات من الـ WAL / binlog و نبعت التغييرات دي للـ
Search index
Cache
Analytics
ML

في مبدأين مهمين جدا بقا
Deterministic + Idempotent
لما تبني derived systems من event log:
Deterministic → نفس المدخلات = نفس النتيجة
Idempotent → لو كررت الحدث مرتين مش يحصل خطأ
وده بيخلي recovery من الأعطال سهل جدًا:
تمسح الـ index
تعيد بناءه من الـ log




Unbundling Databases

دا بيتكلم عن فك النظام الواحد الكبير (Monolithic Database) لحاجات أصغر متخصصة
ف في ال Database هو بيخفي التفاصيل و بيديك قزه كبيره انك تستخدم الكويريز

طيب ليه نفصلهم و كدا؟
في أنظمة حديثة، نحتاج specialized tools لكل وظيفة:
OLTP → transactional DB
Analytics → Data warehouse / Spark
Search → Elasticsearch
Notification / Event streaming → Kafka

مميزات ده  ان
كل واحده متخصصه ف حاجه ف هتديك اداء احسن
و تقدر كمان تعملها ال maintanance عادي
وكل واحد قادر ي scale وحده
العيوب:
تعقيد integration بيخلي ان لازم تنقل البيانات بينهم بشكل صحيح
و بيعمل consistency ف البيانات مشتتة و محتاج mechanisms زي Event Tables, CDC, Message Brokers


